{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "#If you are using Tensorflow 1.x\n",
    "from keras.optimizers import Adam\n",
    "#If you are using Tensorflow 2\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder containing dataset\n",
    "data_path = r'D:\\Downloads\\selfie2anime\\trainB'\n",
    "\n",
    "#how many epochs to run the model\n",
    "epoch = 10\n",
    "\n",
    "#how many epochs between saving your model\n",
    "interval = 2\n",
    "\n",
    "#how many images to train at one time. If batch size is less than 9, alter the save_img function to plot less images\n",
    "batch = 10\n",
    "\n",
    "#if the data has pngs set this to True to remove alpha layer from images\n",
    "png = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "        data = []\n",
    "        small = []\n",
    "        paths = []\n",
    "        #get all files in this folder\n",
    "        for r, d, f in os.walk(data_path):\n",
    "            for file in f:\n",
    "                if '.jpg' in file or 'png' in file:\n",
    "                    paths.append(os.path.join(r, file))\n",
    "        #for each file add normal resolution and low resolution to arrays\n",
    "        for path in paths:\n",
    "            img = Image.open(path)\n",
    "            x = np.array(img.resize((64,64)))\n",
    "            y = np.array(img.resize((128,128)))\n",
    "            if(png):\n",
    "                x = x[...,:3]\n",
    "            data.append(y)\n",
    "            small.append(x)\n",
    "            \n",
    "        #reshaping data to be four dimension required for input to neural network\n",
    "        y_train = np.array(data)\n",
    "        y_train = y_train.reshape(len(data),128,128,3)\n",
    "        x_train = np.array(small)\n",
    "        x_train = x_train.reshape(len(small),64,64,3)\n",
    "        del data\n",
    "        del small\n",
    "        del paths\n",
    "        return y_train, x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGAN():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Shape of high resolution output image\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        # Shape of low resolution input image\n",
    "        self.latent_dim = (64,64,3)\n",
    "\n",
    "        #optimizer (learning rate and beta values)\n",
    "        optimizer = Adam(0.0001, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        generator = self.generator\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=self.latent_dim)\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(128, input_shape=self.latent_dim, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(3, kernel_size=(3,3), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=self.latent_dim)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "    \n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        Y_train, X_train = load_data()\n",
    "\n",
    "        # Rescale to be between 0 & 1\n",
    "        X_train = X_train / 255\n",
    "        Y_train = Y_train / 255\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        # Placeholder for loss function values\n",
    "        g_loss_epochs = np.zeros((epochs, 1))\n",
    "        d_loss_epochs = np.zeros((epochs, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, Y_train.shape[0], batch_size)\n",
    "            imgs = Y_train[idx]\n",
    "\n",
    "            # Generate super resolution images from the random batch of images\n",
    "            gen_imgs = self.generator.predict(X_train[idx])\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(X_train[idx], valid)\n",
    "            \n",
    "            #save loss history\n",
    "            g_loss_epochs[epoch] = g_loss\n",
    "            d_loss_epochs[epoch] = d_loss[0]\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch, X_train, idx)\n",
    "                \n",
    "        return g_loss_epochs, d_loss_epochs\n",
    "\n",
    "    def save_imgs(self, epoch, X_train, idx):\n",
    "        r, c = 3, 3\n",
    "        # Select 9 random images\n",
    "        index = np.random.randint(0, X_train.shape[0], 9)\n",
    "        images = X_train[idx]\n",
    "        # Super resolution the images\n",
    "        gen_imgs = self.generator.predict(images)\n",
    "        gen_imgs = np.array(gen_imgs) * 255\n",
    "        gen_imgs = gen_imgs.astype(int)\n",
    "        # Plot each image\n",
    "        fig=plt.figure(figsize=(20, 20))\n",
    "        for i in range(1, c*r+1):\n",
    "            img = gen_imgs[i-1]\n",
    "            fig.add_subplot(r, c, i)\n",
    "            plt.imshow(img)\n",
    "        fig.savefig(r\"C:\\Users\\Vee\\Desktop\\python\\GAN\\epoch_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "        # save model to .h5 file\n",
    "        self.generator.save(r\"C:\\Users\\Vee\\Desktop\\python\\GAN\\models\\generator\" + str(epoch) + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model and View Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 32769     \n",
      "=================================================================\n",
      "Total params: 273,601\n",
      "Trainable params: 273,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 128, 128, 3)       3459      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 128, 128, 3)       0         \n",
      "=================================================================\n",
      "Total params: 597,379\n",
      "Trainable params: 597,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan = SRGAN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.696746, acc.: 20.00%] [G loss: 0.691277]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vee\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.646467, acc.: 50.00%] [G loss: 0.679452]\n",
      "2 [D loss: 0.606416, acc.: 50.00%] [G loss: 0.642626]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [D loss: 0.613520, acc.: 50.00%] [G loss: 0.586475]\n",
      "4 [D loss: 0.663365, acc.: 50.00%] [G loss: 0.510259]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [D loss: 0.772491, acc.: 50.00%] [G loss: 0.456915]\n",
      "6 [D loss: 0.835647, acc.: 50.00%] [G loss: 0.457943]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 [D loss: 0.753171, acc.: 50.00%] [G loss: 0.657442]\n",
      "8 [D loss: 0.702175, acc.: 55.00%] [G loss: 1.004203]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 [D loss: 0.620197, acc.: 95.00%] [G loss: 1.414992]\n"
     ]
    }
   ],
   "source": [
    "g_loss, d_loss = gan.train(epochs=epoch, batch_size=batch, save_interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_loss)\n",
    "plt.plot(d_loss)\n",
    "plt.title('GAN Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Generator', 'Discriminator'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
